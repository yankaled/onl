{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função é utilizada para reduzir a propogação de erros de \n",
    "arredondamento oriundos da aritmética com pontos flutuantes\n",
    "\"\"\"\n",
    "def truncador(numero):\n",
    "    if(type(numero)) == float:\n",
    "        numero_str = str(numero)\n",
    "        inteira, decimal = numero_str.split('.')\n",
    "        digitos_decimal = list(decimal)    \n",
    "        if len(digitos_decimal) >= 3:\n",
    "            if digitos_decimal[0] == '9' and digitos_decimal[1] == '9' and digitos_decimal[2] == '9':\n",
    "                return math.ceil(numero)\n",
    "        return math.floor(numero*10**3)/(10**3)\n",
    "    else:\n",
    "        return numero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função nos retorna a derivada particial de uma \n",
    "função multivariada em relação à uma variável de índice i\n",
    "\"\"\"\n",
    "def derivada(f, X, i):\n",
    "    h = 10**-10\n",
    "    Xh = []\n",
    "    for j in range(len(X)):\n",
    "        if j != i:  \n",
    "            Xh.append(X[j])\n",
    "        else:\n",
    "            Xh.append(X[i] + h)\n",
    "    return (f(Xh) - f(X))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função nos retorna o gradiente completo de uma função\n",
    "aplicada sobre um determinado ponto\n",
    "\"\"\"\n",
    "def gradiente(f, X):\n",
    "    grad = []\n",
    "    for i in range(len(X)):\n",
    "        grad.append(truncador(derivada(f, X, i)))\n",
    "    return np.array(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função (extra) pode ser usada para verificar se uma matriz é positiva definida,\n",
    "ou seja, pode ser utilizada em verificações de otimalidade (esta função não foi utilizada neste estudo)\n",
    "\"\"\"\n",
    "def cholesky_positiva_semidef(A, n):\n",
    "    l = np.zeros((n, n))\n",
    "    y = np.zeros((n))\n",
    "    X = np.zeros((n))\n",
    "    \n",
    "    if (A[0][0] >= 0):\n",
    "        l[0][0] = math.sqrt(A[0][0])\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    for j in range(1, n):        \n",
    "        l[j][0] = A[j][0]/l[0][0]\n",
    "    \n",
    "    for i in range(1, n-1):\n",
    "        sum_col = 0\n",
    "        for k in range(0, i):\n",
    "            sum_col = sum_col + math.pow(l[i][k], 2)\n",
    "        if A[i][i] >= 0:\n",
    "            l[i][i] = math.sqrt(A[i][i] - sum_col)\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        for j in range(i+1, n):\n",
    "            sum_prod = 0\n",
    "            for k in range(0, i): sum_prod = sum_prod + l[j][k]*l[i][k]\n",
    "            l[j][i] = (A[j][i] - sum_prod)/l[i][i]\n",
    "            \n",
    "    sum_col_last = 0\n",
    "    for k in range(0, n-1): sum_col_last = sum_col_last + math.pow(l[n-1][k], 2) \n",
    "    \n",
    "    if(A[n-1][n-1] >=0 ):\n",
    "        l[n-1][n-1] = math.sqrt(A[n-1][n-1] - sum_col_last)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função calcula a solução de um sistema linear Mx = b, M quadrada, a partir de sua matriz aumentada A = [M | b] \n",
    "\"\"\"\n",
    "def pivotamento_parcial(A, n):\n",
    "    X = [0]*n\n",
    "    nrow = [i for i in range(0, n)]\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        p = i\n",
    "        pivo_maior =  math.fabs(A[nrow[p]][i])\n",
    "        for j in range(i, n):\n",
    "            candidato = math.fabs(A[nrow[j]][i])\n",
    "            if( candidato > pivo_maior ): \n",
    "                pivo_maior = candidato\n",
    "                p = j\n",
    "        \n",
    "        if(A[nrow[p]][i] == 0): return 'Sistema sem solução única'\n",
    "            \n",
    "        if(nrow[i] != nrow[p]):\n",
    "            ncopy = nrow[i]\n",
    "            nrow[i] = nrow[p]\n",
    "            nrow[p] = ncopy\n",
    "        \n",
    "        for j in range(i+1, n):\n",
    "            m = A[nrow[j]][i]/A[nrow[i]][i]\n",
    "            mA_nrowi = [m*a for a in A[nrow[i]]]\n",
    "            \n",
    "            A[nrow[j]] = [a_nrowj - ma_nrowi for a_nrowj, ma_nrowi in zip(A[nrow[j]], mA_nrowi)]\n",
    "            \n",
    "    if(A[nrow[n-1]][n-1] == 0): return 'Sistema sem solução única'\n",
    "    \n",
    "    X[n-1] = A[nrow[n-1]][n]/A[nrow[n-1]][n-1]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        sum_lin = 0\n",
    "        for j in range(i, n):\n",
    "            sum_lin = sum_lin + A[nrow[i]][j]*X[j]\n",
    "        X[i] = ( A[nrow[i]][n] - sum_lin ) / A[nrow[i]][i]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função seguinte retorna uma aproximação para a derivada segunda de uma função em um ponto a partir da seguinte fórmula:\n",
    "\n",
    "$$\n",
    "\\dfrac{f(x_0 + \\hat{h}, y_0 + \\hat{h}) - f(x_0, y_0 + \\hat{h}) - f(x_0  + \\hat{h}, y_0) + f(x_0, y_0)}{\\hat{h}^2}\n",
    "$$\n",
    "\n",
    "Que pode ser obtida através da expansão de:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial^2 f(x_0, y_0)}{\\partial x \\partial y} \\approx \\dfrac{f_{x}(x_0, y_0 + \\hat{h}) - f_{x}(x_0, y_0)}{\\hat{h}}\n",
    "$$\n",
    "\n",
    "Onde $f_{x}(x_0, y_0)$ representa a derivada parcial de $f$ em relação à $x$ no ponto $(x_0, y_0)$, e onde tomamos sempre o mesmo número pequeno $\\hat{h}$ - para facilitar os cálculos.\n",
    "E extensão para funções de mais de duas variáveis é trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retorna a linha i da matriz hessiana H\n",
    "\"\"\"\n",
    "def derivada_segunda(f, X, i, j):\n",
    "    h = 10**-6\n",
    "    Xhh = []\n",
    "    Xh_ = []\n",
    "    Xh = []\n",
    "    for k in range(len(X)):\n",
    "        if k == i and k == j:\n",
    "            Xh.append(X[k] + h)\n",
    "            Xh_.append(X[k] + h)\n",
    "        elif k == i and k != j:\n",
    "            Xh.append(X[k] + h)\n",
    "            Xh_.append(X[k])\n",
    "        elif k != i and k == j:\n",
    "            Xh.append(X[k])\n",
    "            Xh_.append(X[k] + h)\n",
    "        else:\n",
    "            Xh.append(X[k])\n",
    "            Xh_.append(X[k])\n",
    "            \n",
    "    for k in range(len(Xh)):\n",
    "        if k == j:\n",
    "            Xhh.append(Xh[k] + h)\n",
    "        else:\n",
    "            Xhh.append(Xh[k])\n",
    "            \n",
    "    return (f(Xhh) - f(Xh_) -f(Xh) + f(X))/(h**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retorna a matriz hessiana H\n",
    "\"\"\"\n",
    "def hessiana(f, X):\n",
    "    n = len(X)\n",
    "    H = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            H[i][j] = truncador(derivada_segunda(f, X, i, j))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função deve receber a transposta de Z, pois ortogonaliza vetores linha.\n",
    "Args: Z.T, onde o tipo de Z é numpy.ndarray\n",
    "Exemplo:\n",
    "    Z = np.array([[1, 3],[2, 4]])\n",
    "    a, b = gram_schmidt(Z.T)\n",
    "\"\"\"\n",
    "def gram_schmidt(Z_no):\n",
    "    Z_no = np.array(Z_no)\n",
    "    U = []\n",
    "    U.append(list(Z_no[0]))\n",
    "    for i in range(1, len(Z_no)):\n",
    "        u = list(Z_no[i])\n",
    "        soma_projecoes = 0\n",
    "        for j in range(i):\n",
    "            soma_projecoes += (np.dot(Z_no[i], U[j])/np.dot(U[j], U[j]))*np.array(U[j])\n",
    "        u = list(u - soma_projecoes)\n",
    "        U.append(u)\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método do gradiente apresentado por Ana Friedlander é simples, e pode ser implementado com uma recursão e uma regra de parada definida pelo usuário da função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_gradiente(f, Xk, K, K_max, TOL):\n",
    "    if K > K_max:\n",
    "        return False\n",
    "    grad_f = gradiente(f, Xk)\n",
    "    dk = -grad_f\n",
    "    alpha = 0.01\n",
    "    t = 0.5\n",
    "    while( f(Xk + t*dk) > f(Xk) + t*alpha*np.dot(grad_f, dk)):\n",
    "        t = 0.5*t\n",
    "    Xk1 = Xk + t*dk\n",
    "    for i in range(len(Xk)):\n",
    "        if math.fabs(Xk1[i] - Xk[i]) > TOL:\n",
    "            return metodo_gradiente(f, Xk1, K + 1, K_max, TOL)\n",
    "    return Xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No método de newton, devemos resolver o sistema linear:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(X_k) d_k = - \\nabla f(X_k)\n",
    "$$\n",
    "\n",
    "Para encontrarmos a direção de Newton $d_k$. Este é um bom caso de uso para a função de decomposição de cholesky (o método de Newton deve ser utilizado caso a Hessiana de $f$ seja positiva definida). A solução do sistema linear acima é realizada pelo método de eliminação de gauss com pivotamento parcial, que reduz problemas de erros de arredondamento oriundos da aritmética com pontos flutuantes. A resolução de um sistema via eliminação de gauss tem complexidade $O(n^3)$, e o total de computações adicionais pelo pivotamento parcial é pequeno - como pode ser encontrado em Burden, Numerical Analaysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_newton(f, Xk, K, K_max, TOL):\n",
    "    if K > K_max:\n",
    "        return False\n",
    "    H = hessiana(f, Xk)\n",
    "    grad_f = gradiente(f, Xk)\n",
    "    H_aumentada = []\n",
    "    for i in range(len(H)):\n",
    "        H_aumentada.append([])\n",
    "        for j in range(len(H[i])):\n",
    "            H_aumentada[i].append(H[i][j])\n",
    "        H_aumentada[i].append(-grad_f[i])\n",
    "\n",
    "    dk = np.array(pivotamento_parcial(H_aumentada, len(H_aumentada)))\n",
    "    alpha = 0.01\n",
    "    t = 0.5\n",
    "    while( f(Xk + t*dk) > f(Xk) + t*alpha*np.dot(grad_f, dk)):\n",
    "        t = 0.5*t\n",
    "    Xk1 = Xk + t*dk\n",
    "    for i in range(len(Xk)):\n",
    "        if math.fabs(Xk1[i] - Xk[i]) > TOL:\n",
    "            return metodo_newton(f, Xk1, K + 1, K_max, TOL)\n",
    "    return Xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos um problema não linear da forma:\n",
    "\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{sa:} \\ Ax = b\n",
    "$$\n",
    "\n",
    "Onde $A \\in R^{m \\ \\text{x} \\ n}$, $ 1 \\leq m < n$, posto de $A = m$, $f : R^{n} \\to R$, $x \\in R^{n}$ e $b \\in R^{n}$.\n",
    "\n",
    "Na página 57, Ana F. sugere um algoritmo seguindo a direção do gradiente projetado no espaço factível, a partir de uma matriz de projetção $Proj_{Nu(A)}$ que projeta um vetor $v$ no núcleo de $A$. O raciocínio para encontrar a primeira fórmula pode ser encontrado em Luenberguer. De forma simplificada, a ideia é que de $Ad = 0$ - pois a direção factível deve estar no núcleo de $A$ - segue que $d \\perp Im(A^t)$. Daí, qualquer vetor de $R^n$ pode ser escrito como uma combinação do vetor $d$ e de um vetor da forma $A^{t}\\lambda$, onde $\\lambda \\in R^m$. Daí, segue que $-\\nabla f(x) = d + A^{t}\\lambda$, para algum $\\lambda \\in R^m$, pois $\\nabla f(x) \\in R^n$. Multiplicando à esquerda ambos os lados da equação por $A$, temos que $-A\\nabla f(x) = Ad + (AA^{t})\\lambda$. Como $d$ está no núcleo de $A$, segue que $Ad = 0$, e, portanto, isolando o $\\lambda$, obtemos $\\lambda = - (AA^{t})^{-1}A\\nabla f(x)$. \n",
    "\n",
    "O problema de utilizarmos diretamente a matriz $A$, é que a necessidade de calcular a inversa de $AA^{t}$ adiciona um custo computacional excessivo em problemas grandes. Por isso, podemos usar a outra fórmula sugerida por Ana F., ortogonalizando a matriz $Z$ cujas colunas geram $Nu(A)$ através do processo de gram-schmidt. A lógica para encontrar a fórmula sugerida por Ana F. é semelhante à do parágrafo anterior: segue de $Ad = 0 \\Rightarrow \\exists \\gamma \\in R^{n \\text{x} m} \\ \\text{tal que} \\ d = Z\\gamma$ que:\n",
    "$$\n",
    "-\\nabla f(x) = d + A^{t}\\lambda \\Rightarrow -Z^{t}\\nabla f(x) = Z^{t}Z\\gamma + Z^{t}A^{t}\\lambda = Z^{t}Z\\gamma\n",
    "$$ \n",
    "\n",
    "pois $Z\\gamma = d$ e $Z^{t}A^{t}\\lambda = 0$, pois o vetor $A^{t}\\lambda$ é ortogonal ao $Nu(A)$. Daí, basta isolarmos o $\\gamma$ para encontrarmos $\\gamma = -(Z^{t}Z)^{-1}Z^{t}\\nabla f(x)$, e, multiplicando à esquerda por $Z$:\n",
    "\n",
    "$$\n",
    "d = -Z(Z^{t}Z)^{-1}Z^{t}\\nabla f(x)\n",
    "$$\n",
    "\n",
    "Que se torna:\n",
    "\n",
    "$$\n",
    "d = -ZZ^{t}\\nabla f(x)\n",
    "$$\n",
    "\n",
    "Quando Z é ortogonal. Notemos que a regra de parada do algoritmo funciona porque no ponto ótimo, o gradiente da função $f$ será ortogonal ao núcleo de $A$ - o que implica que o gradiente é uma combinação linear das linhas de $A$ - e, portando, sua projeção no núcleo terá norma zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta função recebe a matriz Z, cujas colunas geram o núcleo da matriz de restrições A\n",
    "\"\"\"\n",
    "def metodo_gradiente_projetado(f, Xk, Z, Proj, t, K, K_max, TOL):\n",
    "    if K > K_max:\n",
    "        return False\n",
    "    if K == 0:\n",
    "        if len(Z.T > 1):\n",
    "            Z = np.array(gram_schmidt(Z.T)).T\n",
    "        Proj = np.matmul(Z, Z.T)\n",
    "    grad_f = gradiente(f, Xk)\n",
    "    grad_proj = np.matmul(Proj, grad_f)\n",
    "    if np.linalg.norm(grad_proj) > TOL:\n",
    "        t = t*0.5\n",
    "        return metodo_gradiente_projetado(f, Xk - t*grad_proj, Z, Proj, t, K + 1, K_max, TOL)\n",
    "    return Xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para um breve Estudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(X):\n",
    "    return X[0]**2 + X[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mínimo para essa simples função é em $(0, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.0209531e-11, -4.9008397e-11])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metodo_gradiente(f2, [80, 2], 0, 100, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    return X[0]**3 + X[1]**4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que essa função não tem mínimo, pois ela decresce ao infinito quando seu primeiro termo decresce ao infinito. O resultado é que o algoritmo retorna o resultado da iteração máxima, ou um resultado de módulo grande quando o algoritmo dá passos muito pequenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.59002033e+08,  1.64409680e-01])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metodo_gradiente(f1, [1, 1], 0, 100, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxy(X):\n",
    "    return X[0]**4 + X[1]**2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta função, os algoritmos do gradiente e de newton foram testados, alcançando o mesmo resultado rapidamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.2740371e-08, 0.0000000e+00])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metodo_gradiente(fxy, [1, 2], 0, 50, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.40489905e-02, 2.92831934e-05])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metodo_newton(fxy, [1, 2], 0, 50, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(X):\n",
    "    return X[0]**2 + X[1]**2\n",
    "Z_fx = np.array([[1], [-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método do gradiente projetado funciona bem para este sistema simples, cuja resposta real é mínimo em $(0, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50000004, 0.49999996])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metodo_gradiente_projetado(fx, [1 , 0], Z_fx, [], 1, 0, 3, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
